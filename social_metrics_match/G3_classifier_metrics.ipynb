{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal 3: Investigate overall correlation between quantitative and qualitative metrics with a classification based approach.\n",
    "\n",
    "**3.1** \n",
    "- Which metrics are better to classify correctly the experiment? Use external labels [Good, Mid, Bad]\n",
    "- Train a classifier to predict the goodness of the experiment based on assigned labels using both QM and HM features.\n",
    "\n",
    "**3.2**\n",
    " - Use HM survey data as labels: An overall label [0, 1, 2] assigned by humans for each experiment can be found with clustering or with weighted average\n",
    " - Train a classifier with x = QM and y = HM survey metrics\n",
    "\n",
    "**Ablation (both cases)**: try all the combination of QM to get the best classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments Scenarios**:\n",
    "- \"Passing\"\n",
    "- \"Overtaking\"\n",
    "- \"Crossing 1\"\n",
    "- \"Crossing 2\"\n",
    "- \"Advanced 1\"\n",
    "- \"Advanced 2\"\n",
    "- \"Advanced 3\"\n",
    "- \"Advanced 4\"\n",
    "\n",
    "**Labels**:\n",
    "- \"Good\"\n",
    "- \"Mid\"\n",
    "- \"Bad\"\n",
    "\n",
    "**QM Metrics**\n",
    "- [0] Time to Goal\n",
    "- [1] Path length\n",
    "- [2] Cumulative heading changes\n",
    "- [3] Avg robot linear speed\n",
    "\n",
    "- [4] Social Work \n",
    "- [5] Social Work (per second)\n",
    "- [6] Average minimum distance to closest person\n",
    "- [7] Proxemics: intimate space occupancy\n",
    "- [8] Proxemics: personal space occupancy\n",
    "- [9] Proxemics: social space occupancy\n",
    "- [10] Proxemics: public space occupancy\n",
    "\n",
    "**HM Metrics**\n",
    "- [0] Unobtrusiveness\n",
    "- [1] Friendliness\n",
    "- [2] Smoothness\n",
    "- [3] Avoidance Foresight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import expanduser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lab data path:  /root/Social-Nav-Metrics-Matching/social_metrics_match/data_folder/validation_of_metrics_quantitative_and_lab_qualitative.ods\n",
      "survey data path:  /root/Social-Nav-Metrics-Matching/social_metrics_match/data_folder/qualitative_metrics_survey.xlsx\n",
      "results dir path:  /root/social_metrics_results\n"
     ]
    }
   ],
   "source": [
    "home = expanduser(\"~\")\n",
    "# Load config params for experiments\n",
    "config = yaml.safe_load(open('params.yaml'))['social_metrics_match']\n",
    "\n",
    "lab_data_path = home + config['data']['repo_dir'] + config['data']['lab_data_path']\n",
    "survey_data_path = home + config['data']['repo_dir'] + config['data']['survey_data_path']\n",
    "results_dir = home + config['data']['results_path']\n",
    "print(\"lab data path: \", lab_data_path)\n",
    "print(\"survey data path: \", survey_data_path)\n",
    "print(\"results dir path: \", results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract LAB data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_organization import organize_dict_lab_data, get_all_lab_data_arr, np_extract_exp_lab, np_single_lab_run\n",
    "from utils.data_organization  import organize_dict_survey, weighted_avg_survey_data, get_robotics_knowledge, datacube_qual_survey_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing single run QM shape:(11,), passing single run HM shape: (4,)\n",
      "Passing good QM: [1.02326076e+01 4.55598068e+00 4.04913597e+00 1.99936767e-01\n",
      " 1.99432749e+03 1.81716719e+02 2.37741413e+00 9.28961749e+00\n",
      " 1.23341140e+01 7.28337237e+01 5.54254489e+00],\n",
      "Passing good HM: [0.8 0.8 0.8 1. ]\n",
      "passing QM shape:(11, 3), passing HM shape: (4, 3)\n",
      "All lab QM array: (24, 11), All lab HM array: (24, 4)\n"
     ]
    }
   ],
   "source": [
    "dict_lab_data = organize_dict_lab_data(lab_data_path)\n",
    "\n",
    "# Extract the np arrays of a specific experiments identified by its keys\n",
    "passing_good_QM_array, passing_good_HM_array = np_single_lab_run(dict_lab_data, experiment='Passing', label='Good')\n",
    "print(f\"Passing single run QM shape:{passing_good_QM_array.shape}, passing single run HM shape: {passing_good_HM_array.shape}\")\n",
    "print(f\"Passing good QM: {passing_good_QM_array},\\nPassing good HM: {passing_good_HM_array}\") \n",
    "\n",
    "# Extract the np arrays of a lab scenario (all the 3 runs with different labels), dividing QM and HM\n",
    "passing_QM_array, passing_HM_array = np_extract_exp_lab(dict_lab_data, experiment='Advanced 4', order=True, normalize=True, normalization=\"rescale\")\n",
    "print(f\"passing QM shape:{passing_QM_array.shape}, passing HM shape: {passing_HM_array.shape}\")\n",
    "# print(f\"passing QM: {passing_QM_array},\\npassing HM: {passing_HM_array}\")\n",
    "\n",
    "# Starting from the complete dataframe with lab data, Extract the np arrays of all lab scenarios dividing QM and HM\n",
    "all_lab_QM_array, all_lab_HM_array = get_all_lab_data_arr(dict_lab_data, order=True, normalize=True, normalization=\"rescale\")\n",
    "print(f\"All lab QM array: {all_lab_QM_array.shape}, All lab HM array: {all_lab_HM_array.shape}\")\n",
    "# print(f\"All lab QM array: {all_lab_QM_array}, All lab HM array: {all_lab_HM_array}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SURVEY DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survey weighted avg shape: (24, 4),\n",
      "survey weighted std shape:  (24, 4)\n"
     ]
    }
   ],
   "source": [
    "dict_survey_data = organize_dict_survey(survey_data_path)\n",
    "robot_knowledge_array = get_robotics_knowledge(survey_data_path)\n",
    "\n",
    "# To extract np arrays of all the survey data\n",
    "survey_datacube = datacube_qual_survey_data(dict_survey_data, normalize=True)\n",
    "\n",
    "# To directly extract the average and std: If Weighted average set w_avg=True (use robotics background knowledge as weights)\n",
    "weighted_survey_array_avg, weighted_survey_array_std = weighted_avg_survey_data(dict_survey_data, robot_knowledge_array, w_avg=True, normalize=False)\n",
    "print(f\"survey weighted avg shape: {weighted_survey_array_avg.shape},\\nsurvey weighted std shape:  {weighted_survey_array_std.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24,) [0 1 2 1 1 2 2 0 2 2 1 0 1 2 2 1 1 0 2 0 2 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Encode survey aggregated scores in labels format\n",
    "survey_score = np.rint(np.nanmean(weighted_survey_array_avg, axis=1)).astype(int)\n",
    "survey_score_coded = survey_score.copy()\n",
    "survey_score_coded[survey_score_coded < 3] = 0\n",
    "survey_score_coded[survey_score_coded == 3] = 1\n",
    "survey_score_coded[survey_score_coded > 3] = 2\n",
    "print(survey_score_coded.shape, survey_score_coded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 11) (18, 1)\n",
      "(6, 11) (6,)\n",
      "Y pred:  [1 1 2 2 0 2]\n",
      "Y true:  [1 1 0 2 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define X, y dataset\n",
    "# X = QM metrics combination\n",
    "# y = HM survey aggregatede scores\n",
    "\n",
    "X = all_lab_QM_array\n",
    "y = survey_score_coded\n",
    "\n",
    "X_train = np.vstack([X[:15, :], X[-3:, :]])\n",
    "y_train = np.vstack([y[:15, None], y[-3:, None]])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "X_test = X[15:21, :]\n",
    "y_test = y[15:21]\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Define Decision Tree Classifier\n",
    "print(\"Decision Tree Classifier\")\n",
    "\n",
    "# Define RandomForest Classifier\n",
    "print(\"Random Forest Classifier\")\n",
    "rf_clf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', max_depth=2, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "print(\"Y pred: \", y_pred)\n",
    "print(\"Y true: \", y_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Define SVM Classifier\n",
    "print(\"SVM Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch for hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m bootstrap \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[1;32m      6\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_estimators,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_depth,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m\"\u001b[39m: min_samples_leaf,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m\"\u001b[39m: bootstrap,\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 13\u001b[0m rf \u001b[38;5;241m=\u001b[39m \u001b[43mRandomForestRegressor\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     15\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mrf, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, Y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "n_estimators = [10, 25]\n",
    "max_depth = [2, 10, 25]\n",
    "min_samples_leaf = [2]\n",
    "criterion = ['gini', 'entropy']\n",
    "bootstrap = [True, False]\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": n_estimators,\n",
    "    \"max_depth\": max_depth,\n",
    "    \"min_samples_leaf\": min_samples_leaf,\n",
    "    \"bootstrap\": bootstrap,\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "rf_model = GridSearchCV(estimator=rf, param_grid=param_grid, cv=2, verbose=10, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, Y_train)\n",
    "\n",
    "print(\"Using hyperparameters --> \\n\", rf_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
