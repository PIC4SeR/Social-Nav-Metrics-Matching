{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal 3: Investigate overall correlation between quantitative and qualitative metrics with a classification based approach.\n",
    "\n",
    "**3.1** \n",
    "- Which metrics are better to classify correctly the experiment? Use external labels [Good, Mid, Bad]\n",
    "- Train a classifier to predict the goodness of the experiment based on assigned labels using both QM and HM features.\n",
    "\n",
    "**3.2**\n",
    " - Use HM survey data as labels: An overall label [0, 1, 2] assigned by humans for each experiment can be found with clustering or with weighted average\n",
    " - Train a classifier with x = QM and y = HM survey metrics\n",
    "\n",
    "**Ablation (both cases)**: try all the combination of QM to get the best classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiments Scenarios**:\n",
    "- \"Passing\"\n",
    "- \"Overtaking\"\n",
    "- \"Crossing 1\"\n",
    "- \"Crossing 2\"\n",
    "- \"Advanced 1\"\n",
    "- \"Advanced 2\"\n",
    "- \"Advanced 3\"\n",
    "- \"Advanced 4\"\n",
    "\n",
    "**Labels**:\n",
    "- \"Good\"\n",
    "- \"Mid\"\n",
    "- \"Bad\"\n",
    "\n",
    "**QM Metrics**\n",
    "- [0] Social Work\n",
    "- [1] Social Work (per second)\n",
    "- [2] Time to Goal\n",
    "- [3] Path length\n",
    "- [4] Average minimum distance to closest person\n",
    "- [5] Proxemics: intimate space occupancy\n",
    "- [6] Proxemics: personal space occupancy\n",
    "- [7] Proxemics: social space occupancy\n",
    "- [8] Proxemics: public space occupancy\n",
    "- [9] Cumulative heading changes\n",
    "- [10] Avg robot linear speed\n",
    "\n",
    "**HM Metrics**\n",
    "- [0] Unobtrusiveness\n",
    "- [1] Friendliness\n",
    "- [2] Smoothness\n",
    "- [3] Avoidance Foresight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import expanduser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = expanduser(\"~\")\n",
    "# Load config params for experiments\n",
    "config = yaml.safe_load(open('params.yaml'))['social_metrics_match']\n",
    "\n",
    "lab_data_path = home + config['data']['repo_dir'] + config['data']['lab_data_path']\n",
    "survey_data_path = home + config['data']['repo_dir'] + config['data']['survey_data_path']\n",
    "results_dir = home + config['data']['results_path']\n",
    "print(\"lab data path: \", lab_data_path)\n",
    "print(\"survey data path: \", survey_data_path)\n",
    "print(\"results dir path: \", results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract LAB data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_organization import organize_dict_lab_data, get_all_lab_data_arr, np_extract_exp_lab, np_single_lab_run\n",
    "from utils.data_organization  import organize_dict_survey, weighted_avg_survey_data, get_robotics_knowledge, datacube_qual_survey_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_lab_data = organize_dict_lab_data(lab_data_path)\n",
    "\n",
    "# Extract the np arrays of a specific experiments identified by its keys\n",
    "passing_good_QM_array, passing_good_HM_array = np_single_lab_run(dict_lab_data, experiment='Passing', label='Good')\n",
    "print(f\"Passing single run QM shape:{passing_good_QM_array.shape}, passing single run HM shape: {passing_good_HM_array.shape}\")\n",
    "print(f\"Passing good QM: {passing_good_QM_array},\\nPassing good HM: {passing_good_HM_array}\") \n",
    "\n",
    "# Extract the np arrays of a lab scenario (all the 3 runs with different labels), dividing QM and HM\n",
    "passing_QM_array, passing_HM_array = np_extract_exp_lab(dict_lab_data, experiment='Advanced 4', order=False, normalize=True, normalization=\"rescale\")\n",
    "print(f\"passing QM shape:{passing_QM_array.shape}, passing HM shape: {passing_HM_array.shape}\")\n",
    "# print(f\"passing QM: {passing_QM_array},\\npassing HM: {passing_HM_array}\")\n",
    "\n",
    "# Starting from the complete dataframe with lab data, Extract the np arrays of all lab scenarios dividing QM and HM\n",
    "all_lab_QM_array, all_lab_HM_array = get_all_lab_data_arr(dict_lab_data, normalize=True, normalization=\"rescale\")\n",
    "print(f\"All lab QM array: {all_lab_QM_array.shape}, All lab HM array: {all_lab_HM_array.shape}\")\n",
    "# print(f\"All lab QM array: {all_lab_QM_array}, All lab HM array: {all_lab_HM_array}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SURVEY DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_survey_data = organize_dict_survey(survey_data_path)\n",
    "robot_knowledge_array = get_robotics_knowledge(survey_data_path)\n",
    "\n",
    "# To extract np arrays of all the survey data\n",
    "survey_datacube = datacube_qual_survey_data(dict_survey_data, normalize=True)\n",
    "\n",
    "# To directly extract the average and std: If Weighted average set w_avg=True (use robotics background knowledge as weights)\n",
    "weighted_survey_array_avg, weighted_survey_array_std = weighted_avg_survey_data(dict_survey_data, robot_knowledge_array, w_avg=True,normalize=True)\n",
    "print(f\"survey weighted avg shape: {weighted_survey_array_avg.shape},\\nsurvey weighted std shape:  {weighted_survey_array_std.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
